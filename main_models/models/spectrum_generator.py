"""
Capsule autoencoder implementation.
"""


from __future__ import absolute_import
from __future__ import division
from __future__ import print_function


import collections
import sonnet as snt
import tensorflow.compat.v1 as tf
#tf.experimental.output_all_intermediates(True)
tf.disable_eager_execution()
import fastdtw
import tensorflow_probability as tfp
import tfpyth as tp
from .layers import capsule as _capsule
from Loss import *
from .layers.model import Model
#from models.dilate_loss import dilate_loss
#from models.tf_dtw1 import *
tfd = tfp.distributions


class MaterialCapsule(snt.AbstractModule):
  """ OCAE Capsule decoder for constellations."""

  def __init__(self, n_caps, n_caps_dims, n_votes, **capsule_kwargs):
    """Builds the module.
    Args:
      n_caps: int, number of capsules.
      n_caps_dims: int, number of capsule coordinates.
      n_votes: int, number of votes generated by each capsule.
      **capsule_kwargs: kwargs passed to capsule layer.
    """
    super(MaterialCapsule, self).__init__()
    self._n_caps = n_caps
    self._n_caps_dims = n_caps_dims
    self._n_votes = n_votes
    self._capsule_kwargs = capsule_kwargs

  def _build(self, mol_h, presence=None, state=None):
    """Builds the module.
    Args:
      h: Tensor of encodings of shape [B, n_enc_dims].
      x: Tensor of inputs of shape [B, n_points, n_input_dims]
      presence: Tensor of shape [B, n_points, 1] or None; if it exists, it
        indicates which input points exist.
    Returns:
      A bunch of stuff.
    """
    batch_size = tf.shape(mol_h)[0]

    capsule = _capsule.CapsuleLayer(self._n_caps, self._n_caps_dims, self._n_caps,
                                    **self._capsule_kwargs)

    res = capsule(mol_h)
    vote_shape = [batch_size, self._n_caps, self._n_caps, 6]
    res.vote = tf.reshape(res.vote[Ellipsis, :-1, :], vote_shape)

    votes, scale, vote_presence_prob, \
    raw_caps_params = res.vote, res.scale, res.vote_presence, res.raw_caps_params

    votes = tf.reshape(votes, [batch_size, 128, 96*6])
    likelihood = _capsule.CapsuleLikelihood(votes, scale, vote_presence_prob, raw_caps_params)
    ll_res = likelihood(mol_h, presence = vote_presence_prob, state = state)
    #res.update(ll_res._asdict())

    return votes, scale, vote_presence_prob, raw_caps_params


class ClusterCapsule(Model):
  """Capsule autoencoder."""

  OutputTuple = collections.namedtuple('Cluster_Capsule',
                                       ('rec_ll '
                                        'log_prob '
                                        'dynamic_weights_l2 '
                                        'primary_caps_l1 '
                                        'posterior_within_sparsity_loss '
                                        'posterior_between_sparsity_loss '
                                        'prior_within_sparsity_loss '
                                        'prior_between_sparsity_loss '
                                        'weight_decay_loss '
                                        'posterior_cls_xe '
                                        'prior_cls_xe '
                                        'best_pre_loss '
                                        'votes '
                                        'mass_explained_by_capsule '
                                        'caps_presence_prob '))

  def __init__(
      self,
      primary_encoder,
      primary_decoder,
      encoder,
      decoder,
      input_key,
      vote_type='soft',
      pres_type='soft',
      stop_grad_caps_inpt = True,
      stop_grad_caps_target = True,
      feed_templates = True,
      label_key=None,
      n_classes=None,
      dynamic_l2_weight=10.,
      caps_ll_weight=1.0,
      img_summaries=False,
      prior_sparsity_loss_type='kl',
      prior_within_example_sparsity_weight=1.,
      prior_between_example_sparsity_weight=1.,
      prior_within_example_constant=0.,
      posterior_sparsity_loss_type='kl',
      posterior_within_example_sparsity_weight=10.,
      posterior_between_example_sparsity_weight=10.,
      primary_caps_sparsity_weight=0,
      weight_decay=0.,
          k_eig=0,
      prep='none',):

    super(ClusterCapsule, self).__init__()
    self._stop_grad_caps_inpt = stop_grad_caps_inpt
    self._stop_grad_caps_target = stop_grad_caps_target
    self._feed_templates = feed_templates
    self._primary_encoder = primary_encoder
    self._primary_decoder = primary_decoder
    self._encoder = encoder
    self._decoder = decoder
    self._vote_type = vote_type
    self._pres_type = pres_type
    self._input_key = input_key
    self._label_key = label_key
    self._n_classes = n_classes
    self.k_eig = k_eig

    self._dynamic_l2_weight = dynamic_l2_weight
    self._caps_ll_weight = caps_ll_weight
    self._vote_type = vote_type
    self._pres_type = pres_type
    self._img_summaries = img_summaries
    self._prior_sparsity_loss_type = prior_sparsity_loss_type
    self._prior_within_example_sparsity_weight = prior_within_example_sparsity_weight
    self._prior_between_example_sparsity_weight = prior_between_example_sparsity_weight
    self._prior_within_example_constant = prior_within_example_constant
    self._posterior_sparsity_loss_type = posterior_sparsity_loss_type
    self._posterior_within_example_sparsity_weight = posterior_within_example_sparsity_weight
    self._posterior_between_example_sparsity_weight = posterior_between_example_sparsity_weight
    self._primary_caps_sparsity_weight = primary_caps_sparsity_weight
    self._weight_decay = weight_decay

    self._prep = prep

  def _build(self, atom, bond, atom_1, bond_1, atom_2, bond_2, atom_3, bond_3,# multi clusters
             element_weight, cry_vec, u1_, u1_1, u1_2, u1_3, atom_index, x6, real_bg):

    # cluster_1
    atom_bond = tf.concat([tf.squeeze(atom, 0), bond, tf.squeeze(u1_, 0)], -1) #(1, 5*N, length) The 2D is atom number of 5-cluster
    # cluster_2
    atom_bond_1 = tf.concat([tf.squeeze(atom_1, 0), bond_1, tf.squeeze(u1_1, 0)], -1)
    # cluster_3
    atom_bond_2 = tf.concat([tf.squeeze(atom_2, 0), bond_2, tf.squeeze(u1_2, 0)], -1)
    # cluster_4
    atom_bond_3 = tf.concat([tf.squeeze(atom_3, 0), bond_3, tf.squeeze(u1_3, 0)], -1)

    batch_size = tf.shape(atom_bond)[0]

    primary_caps = self._primary_encoder(atom_bond)
    primary_caps_1 = self._primary_encoder(atom_bond_1)
    primary_caps_2 = self._primary_encoder(atom_bond_2)
    primary_caps_3 = self._primary_encoder(atom_bond_3)
    # Presence
    cluster_pres = primary_caps.presence # (N,  1)
    cluster_pres_1 = primary_caps_1.presence # (N,  1)
    cluster_pres_2 = primary_caps_2.presence # (N,  1)
    cluster_pres_3 = primary_caps_3.presence # (N,  1)

    expanded_pres_1 = tf.expand_dims(cluster_pres, -1, name = 'cap_prob_1')
    expanded_pres_2 = tf.expand_dims(cluster_pres_1, -1, name = 'cap_prob_2')
    expanded_pres_3 = tf.expand_dims(cluster_pres_2, -1, name = 'cap_prob_3')
    expanded_pres_4 = tf.expand_dims(cluster_pres_3, -1, name = 'cap_prob_4')

    # skip connection from the img to the higher level capsule
    pose_pres_fea = tf.concat([primary_caps.pose, primary_caps.feature], -1, name = 'pose_pres_fea')
    pose_pres_fea_1 = tf.concat([primary_caps_1.pose, primary_caps_1.feature], -1, name = 'pose_pres_fea_1')
    pose_pres_fea_2 = tf.concat([primary_caps_2.pose, primary_caps_2.feature], -1, name = 'pose_pres_fea_2')
    pose_pres_fea_3 = tf.concat([primary_caps_3.pose, primary_caps_3.feature], -1, name = 'pose_pres_fea_3')

    pose_pres_fea = snt.Linear(192)(pose_pres_fea)
    pose_pres_fea_1 = snt.Linear(192)(pose_pres_fea_1)
    pose_pres_fea_2 = snt.Linear(192)(pose_pres_fea_2)
    pose_pres_fea_3 = snt.Linear(192)(pose_pres_fea_3)

    pose_pres_fea = 0.5*pose_pres_fea + 0.5*atom_bond
    pose_pres_fea_1 = 0.5*pose_pres_fea_1 + 0.5*atom_bond_1
    pose_pres_fea_2 = 0.5*pose_pres_fea_2 + 0.5*atom_bond_2
    pose_pres_fea_3 = 0.5*pose_pres_fea_3 + 0.5*atom_bond_3

    def get_gaussian_paras(vec, n = 64):
        gau_shapes = (
            [n],  # sigma
            [n],  # mu
            [n],  # coef
        )
        #vec = tf.expand_dims(vec, -1)
        splits = [np.prod(i).astype(np.int32) for i in gau_shapes]
        n_outputs = sum(splits)

        gaussian_h = snt.Linear(256)(vec)
        gaussian_h = snt.Linear(n_outputs)(gaussian_h)

        out_sigma, out_mu, out_alpha = self.get_normal_coef(gaussian_h)

        return out_sigma, out_mu, out_alpha


    clu_cap_1_sigma, clu_cap_1_mu, clu_cap_1_alpha = get_gaussian_paras(pose_pres_fea,   n = 14)
    clu_cap_2_sigma, clu_cap_2_mu, clu_cap_2_alpha = get_gaussian_paras(pose_pres_fea_1, n = 14)
    clu_cap_3_sigma, clu_cap_3_mu, clu_cap_3_alpha = get_gaussian_paras(pose_pres_fea_2, n = 14)
    clu_cap_4_sigma, clu_cap_4_mu, clu_cap_4_alpha = get_gaussian_paras(pose_pres_fea_3, n = 14)

    cry_vec0 = snt.Linear(256)(tf.squeeze(cry_vec, 0))
    cry_sigma, cry_mu, cry_alpha = get_gaussian_paras(cry_vec0, n = 14)

    cap_gau_1 = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(
        probs=clu_cap_1_alpha), components_distribution=tfd.Normal(
        loc=clu_cap_1_mu, scale=clu_cap_1_sigma))

    cap_gau_2 = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(
        probs=clu_cap_2_alpha), components_distribution=tfd.Normal(
        loc=clu_cap_2_mu, scale=clu_cap_2_sigma))

    cap_gau_3 = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(
        probs=clu_cap_3_alpha), components_distribution=tfd.Normal(
        loc=clu_cap_3_mu, scale=clu_cap_3_sigma))

    cap_gau_4 = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(
        probs=clu_cap_4_alpha), components_distribution=tfd.Normal(
        loc=clu_cap_4_mu, scale=clu_cap_4_sigma))

    cry_gau = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(
        probs=cry_alpha), components_distribution=tfd.Normal(
        loc=cry_mu, scale=cry_sigma))

    batch_size = tf.shape(clu_cap_1_sigma)[0]
    batch_shape = [204]
    ind = tf.range(-40, 266, 1, dtype = tf.float32) / 20
    ind1 = tf.expand_dims(ind, 0)
    ind2 = tf.tile(ind1, [batch_size, 1])
    gau_sample_1 = cap_gau_1.prob(ind2)
    gau_sample_2 = cap_gau_2.prob(ind2)
    gau_sample_3 = cap_gau_3.prob(ind2)
    gau_sample_4 = cap_gau_4.prob(ind2)

    cry_size = tf.shape(tf.squeeze(cry_vec, 0))[0]
    ind3 = tf.tile(ind1, [cry_size, 1])
    cry_sample = cry_gau.prob(ind3)
    cry_sample = tf.nn.relu(cry_sample, name='mol_cry_caps')

    gau_sample_1 = tf.nn.relu(gau_sample_1, name="gau_sample_1")
    gau_sample_2 = tf.nn.relu(gau_sample_2, name="gau_sample_2")
    gau_sample_3 = tf.nn.relu(gau_sample_3, name="gau_sample_3")
    gau_sample_4 = tf.nn.relu(gau_sample_4, name="gau_sample_4")

    def maxminnorm(vec):
        min = tf.expand_dims(tf.reduce_min(vec, -1), -1)
        max = tf.expand_dims(tf.reduce_max(vec, -1), -1) + 0.01
        up_vec = (vec - min) / (max - min)
        return up_vec

    #cry_sample = tf.nn.relu(cry_sample, name="mol_cry_caps")

    gau_sample_1 = tf.multiply(gau_sample_1, expanded_pres_1)
    gau_sample_2 = tf.multiply(gau_sample_2, expanded_pres_2)
    gau_sample_3 = tf.multiply(gau_sample_3, expanded_pres_3)
    gau_sample_4 = tf.multiply(gau_sample_4, expanded_pres_4)

    h = self._atom2mol(tf.expand_dims(gau_sample_1, -1), x6)  # (N, n_o_caps, 256)
    h = tf.squeeze(h, -1, name='mol_clus')
    #h = maxminnorm(h)

    h_1 = self._atom2mol(tf.expand_dims(gau_sample_2, -1), x6)  # (N, n_o_caps, 256)
    h_1 = tf.squeeze(h_1, -1, name='mol_clus_1')
    #h_1 = maxminnorm(h_1)

    h_2 = self._atom2mol(tf.expand_dims(gau_sample_3, -1), x6)  # (N, n_o_caps, 256)
    h_2 = tf.squeeze(h_2, -1, name='mol_clus_2')
    #h_2 = maxminnorm(h_2)

    h_3 = self._atom2mol(tf.expand_dims(gau_sample_4, -1), x6)  # (N, n_o_caps, 256)
    h_3 = tf.squeeze(h_3, -1, name='mol_clus_3')
    #h_3 = maxminnorm(h_3)
    #cry_sample = maxminnorm(cry_sample)

    alpha_1 = snt.Linear(1, name="mol_alpha_1")(h)
    alpha_2 = snt.Linear(1, name="mol_alpha_2")(h_1)
    alpha_3 = snt.Linear(1, name="mol_alpha_3")(h_2)
    alpha_4 = snt.Linear(1, name="mol_alpha_4")(h_3)
    alpha_cry = tf.nn.sigmoid(snt.Linear(1)(cry_sample), name="mol_alpha_cry")

    alpha_sets = tf.concat([alpha_1, alpha_2, alpha_3, alpha_4], -1, name = 'alpha_sets')
    alpha_sets = tf.nn.softmax(alpha_sets, name = 'gau_alpha')
    up_alpha_1, up_alpha_2, up_alpha_3, up_alpha_4= tf.split(alpha_sets, 4, -1)

    #residual_mol = up_alpha_1 * h + up_alpha_2 * h_1 +up_alpha_3 * h_2 +up_alpha_4 * h_3 + alpha_cry * cry_sample
    residual_mol = 0.36 * h + 0.36 * h_1 + 0.36 * h_2 + 0.36 * h_3 + 0 * cry_sample

    resample_pool = tf.nn.avg_pool1d(tf.expand_dims(residual_mol, -1),6,6,padding = 'SAME', name = 'resample_pool')
    #pre_1= snt.Linear(51)(pre_0)

    fea_pre = tf.squeeze(resample_pool, -1)
    fea_pre = maxminnorm(fea_pre)

    #Loss function
    real_bg = tf.squeeze(real_bg, 0)

    # dilate loss from tensorflow to pytorch

    def batch_distance(X, Y, metric="L1"):
        assert metric == "L1", "wrong metric value !"

        N, T1, d = tf.shape(X)[0], tf.shape(X)[1], tf.shape(X)[2]
        T2 = tf.shape(Y)[1]

        X = tf.reshape(tf.tile(X, [1, 1, T2]), (N * T1 * T2, d))
        Y = tf.reshape(tf.tile(Y, [1, T1, 1]), (N * T1 * T2, d))

        res = tf.math.square(X - Y)
        res = tf.reduce_sum(res, axis=-1)
        res = tf.cast(tf.reshape(res, [N, T1, T2]), tf.float32)

        raw_res = tf.math.abs(X - Y)
        raw_res = tf.reduce_sum(raw_res, axis=-1)
        raw_res = tf.cast(tf.reshape(raw_res, [N, T1, T2]), tf.float32)

        return res, raw_res

    x = tf.expand_dims(fea_pre, -1)
    y = tf.expand_dims(real_bg, -1)

    N, T1 = tf.shape(x)[0], tf.shape(x)[1]
    T2 = tf.shape(y)[1]

    #  获取欧式距离矩阵
    delta_matrix, raw_delta_matrix = batch_distance(x, y, metric="L1")
    delta_matrix1 = tf.reshape(delta_matrix, (N, -1))
    delta_matrix1 = tf.transpose(delta_matrix1, (1, 0))
    d_array = tf.TensorArray(tf.float32, size=(T1 * T2), clear_after_read=False)
    d_array = d_array.unstack(delta_matrix1)

    r_array = tf.TensorArray(tf.float32, size=(T1 + 1) * (T2 + 1), clear_after_read=False)
    r_array = r_array.write(0, tf.zeros(shape=(N,)))

    def cond_boder_x(idx, array):
        return idx < T1 + 1

    def cond_boder_y(idx, seq_len):
        return idx < T2 + 1

    def body_border_x(idx, array):
        array = array.write(tf.cast(idx * (T2 + 1), tf.int32), 1000000 * tf.ones(shape=(N,)))
        return idx + 1, array

    def body_border_y(idx, array):
        array = array.write(tf.cast(idx, tf.int32), 1000000 * tf.ones(shape=(N,)))
        return idx + 1, array

    _, r_array = tf.while_loop(cond_boder_x, body_border_x, (1, r_array))
    _, r_array = tf.while_loop(cond_boder_y, body_border_y, (1, r_array))

    def cond(idx, array):
        return idx < (T1 + 1) * (T2 + 1)

    def body(idx, array):
        i = tf.cast(tf.divide(idx, T2 + 1), tf.int32)  # 行号
        j = tf.math.floormod(tf.cast(idx, tf.int32), T2 + 1)  # 列号

        def inner_func_v1():
            """ Parallel Tacotron2's version """
            gamma = 0.001
            warp = 0

            b = (array.read((i - 1) * (T2 + 1) + (j - 1)))
            a = (warp + array.read((i - 1) * (T2 + 1) + (j)))
            c = (warp + array.read((i) * (T2 + 1) + (j - 1)))

            a /= -gamma
            b /= -gamma
            c /= -gamma

            # maximum, minimum
            max1 = tf.math.maximum(a, b)
            max2 = tf.math.maximum(max1, c)

            z1 = (a - max2)
            z2 = (b - max2)
            z3 = (c - max2)

            # z1 = -1./gamma * (array.read((i-1)*(T2+1)+(j-1)) + r_array.read((i-1)*(T2+1)+(j-1)))
            # z2 = -1./gamma * (warp+array.read((i-1)*(T2+1)+(j)) + r_array.read((i-1)*(T2+1)+(j)))
            # z3 = -1./gamma * (warp+array.read((i)*(T2+1)+(j-1)) + r_array.read((i)*(T2+1)+(j-1)))
            soft_min_value = -gamma * (tf.math.reduce_logsumexp([z1, z2, z3], axis=0) + max2)
            # mn = delta_matrix[:, j-1, j-1]
            # mn = d_array.read(i * (T2 + 1) + j - T2 - i - 1)
            # mn = tf.squeeze(mn, -1)
            mn = tf.gather_nd(delta_matrix1, [i * (T2 + 1) + j - T2 - i - 1])
            mn = tf.reshape(mn, [-1])
            # mn = tf.map_fn(tf.diag_part, delta_matrix, parallel_iterations = N)
            soft_min_value = mn + soft_min_value
            r_value = tf.cast(soft_min_value, tf.float32)

            return array.write(idx, tf.cast(r_value, tf.float32))

        def outer_func():
            return array

        array = tf.cond(tf.less(i, 1) | tf.less(j, 1),
                        true_fn=outer_func,
                        false_fn=inner_func_v1)

        return idx + 1, array

    _, r_array = tf.while_loop(cond, body, (0, r_array))
    r_matrix = r_array.stack()
    r_matrix = tf.transpose(r_matrix, [1, 0])

    r_matrix1 = tf.reshape(r_matrix, (N, T1 + 1, T2 + 1))

    # r_matrix_v1 = tf.identity(tf.cast(r_matrix1, tf.float32), "r_matrix_v1")
    dilate_value = r_matrix1[:, -1, -1]
    dilate_value = tf.reduce_mean(dilate_value)
    dilate_value = tf.stop_gradient(dilate_value)

    epsilon = 0
    dlog_loss = tf.reduce_mean(tf.abs(tf.exp(real_bg + epsilon) - tf.exp(fea_pre + epsilon)))
    mae_loss = tf.reduce_mean(tf.abs(real_bg - fea_pre))

    total_loss = 0.5 * dilate_value + 0.5 * mae_loss

    mse = pred_loss_1(fea_pre, real_bg)
    fea_pre = fea_pre+0.01
    real_bg = real_bg+0.01
    mape = tf.reduce_mean(tf.abs((real_bg-fea_pre) / real_bg))

    phi = 0.1
    #log_loss = phi * loss

    likelihood_loss = tf.stack([mse, mse], name = 'likelihood_loss')
    mse_loss        = tf.stack([total_loss, mse], name = 'mse_loss')
    rec_mse_loss    = tf.stack([mse, mse], name = 'rec_mse_loss')

    return mse_loss, fea_pre


  def _mse_loss(self, data, res):
      mape, mse = tf.unstack(res)

      alpha = 0
      phi = 1
      loss = phi * mape + alpha * mse
      return loss


  def _atom2mol(self, inp, ind):

      ind = tf.reshape(ind, (-1,))
      #maxes = tf.math.segment_max(inp[0], ind)
      a = int(inp.shape[1])
      b = int(inp.shape[2])
      inp = tf.reshape(inp, [-1, a * b])
      out = tf.math.segment_sum(inp, ind)
      out = tf.reshape(out, [-1, a, b], name = "atom2mol")

      return out

  def get_mixture_coef(self, output):

      out_pi, out_sigma, out_mu = tf.split(output, 3, -1)

      max_pi = tf.reduce_max(out_pi, 1, keep_dims=True)
      out_pi = tf.subtract(out_pi, max_pi)

      out_pi = tf.exp(out_pi)

      normalize_pi = tf.reciprocal(tf.reduce_sum(out_pi, 1, keep_dims=True))
      out_pi = tf.multiply(normalize_pi, out_pi)

      out_sigma = tf.exp(out_sigma)

      return out_pi, out_sigma, out_mu

  def get_normal_coef(self, output):

      out_sigma, out_mu, out_alpha = tf.split(output, 3, -1)

      out_sigma = tf.exp(out_sigma, name = 'out_sigma')
      out_alpha = tf.nn.softmax(out_alpha, name = 'out_alpha')
      #out_mu = tf.nn.relu(out_mu, name = 'out_mu')


      out_alpha = tf.expand_dims(out_alpha, 1)
      out_alpha = tf.tile(out_alpha, [1, 306, 1])
      out_mu = tf.expand_dims(out_mu, 1)
      out_mu = tf.tile(out_mu, [1, 306, 1])
      out_sigma = tf.expand_dims(out_sigma, 1)
      out_sigma = tf.tile(out_sigma, [1, 306, 1])

      return out_sigma, out_mu, out_alpha






